<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We explore how video and text representations align, finding that more data at test time‚Äîmore frames and more captions‚Äîleads to stronger alignment, which in turn correlates with better performance on downstream tasks.">
  <meta name="keywords" content="videos, LLMs, video representation, text alignment, representation learning, multimodal learning, video understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Dynamic Reflections: Probing Video Representations with Text Alignment</title>

  <!-- Font family taken from https://ai.stanford.edu/~sharonal/, thanks in advance for the great taste!-->
  <style>
    * {
      font-family: "TeXGyrePagella", "Palatino Linotype", "Book Antiqua", Palatino, serif !important;
    }

  </style>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css" integrity="sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js" integrity="sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ" crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          // customised options
          // ‚Ä¢ auto-render specific keys, e.g.:
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          // ‚Ä¢ rendering keys, e.g.:
          throwOnError : false
        });
    });

    document.addEventListener('DOMContentLoaded', function() {
      bulmaCarousel.attach('#image-carousel', {
          slidesToScroll: 1,
          slidesToShow: 1,
          loop: true,
          autoplay: false,
      });
    });
</script>
<style>
    .results-carousel .carousel-nav {
        top: calc(50% + 2rem); /* Move arrows down */
    }
    .results-carousel .carousel-nav.is-left {
        left: -6rem; /* Move left arrow further out */
    }
    .results-carousel .carousel-nav.is-right {
        right: -6rem; /* Move right arrow further out */
    }
    .carousel .carousel-pagination {
        margin-top: 8rem; /* Add space between image and pagination dots */
    }
    .results-carousel .item {
        border: none !important;
        box-shadow: none !important;
    }
</style>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Dynamic Reflections: Probing Video Representations with Text Alignment</h1>

          <!-- <h2 class="is-size-5 is-centered"><strong>ICML 2025</strong></h2> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tylerzhu.com">Tyler Zhu</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://tengdahan.github.io/">Tengda Han</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://geometry.stanford.edu">Leonidas Guibas</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Viorica PƒÉtrƒÉucean<sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.lix.polytechnique.fr/~maks/">Maks Ovsjanikov</a><sup>2</sup>
            </span>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Princeton University</span>,
            <span class="author-block"><sup>2</sup>Google DeepMind</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Work done while at Google DeepMind</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2511.02767"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/eBmAAdWrrAs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <!-- <a href="https://github.com/google-deepmind/video-prh" -->
                <a 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming)</span>
                </a>
              </span>
              <!-- Slides Link. -->
              <!-- <span class="link-block">
                <a href="./static/cvpr23_pareprop_slides.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span> -->
              <!-- Poster Link. -->
              <span class="link-block">
                <!-- <a href="./static/merv_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-clipboard"></i>
                  </span>
                  <span>Poster</span>
                </a>
 -->              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="column has-text-centered">
        <img src="static/images/teaser.png" style="width:80%; display: block; margin: auto;" />
      </div>
      <h2 class="subtitle has-text-centered">
        Recent work has shown that models trained independently on distinct data, like images and text, learn surprisingly similar internal structures. 
        We show that the same models (and more) can achieve dramatically improved alignment with more data at test time, i.e. videos and multiple sets of captions. 
      </h2>
      <h2 class="subtitle has-text-centered">
        <div class="box has-background-info-light">
          Strong alignment was in these models all along; the data was just too impoverished to reveal it!
        </div>
      </h2>

        <!-- We conduct the first comprehensive study of video and text alignment: more data at test time dramatically improves alignment, completely training-free! -->
        <!-- We extend this analysis to dynamic inputs, namely video and multiple sets of captions, finding that richer data significantly improves alignment. -->
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types.
            While significant progress has been made in aligning images with text, the temporal nature of <i>video</i> data remains largely unexplored in this context. 
            In this work, we conduct the <b>first comprehensive study of video-text representation alignment</b>, probing the capabilities of modern video and language encoders. 
            Our findings reveal several key insights. 
            First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data <i>provided at test time</i>, especially when using state-of-the-art video encoders. 
            We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations.
            Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to <i>general-purpose video</i> representation and understanding.
            Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. 
            Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data.
        <!-- <figcaption>Our work shows that video representations are strongly aligned with text. We find that large video models (üîµ) and image models that average over frames (üü†) achieve the highest alignment and downstream performance, outperforming simple image-only models (üü¢).</figcaption> -->
          </p>
 
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Background: The Platonic Representation Hypothesis</h2>
         <div class="content has-text-justified">
          <p>
            As models are trained on larger datasets with similar architectures, are they learning similar representations of the world? 
            This idea was posited in <a href="#huh2024platonic">[1]</a> as the <strong>Platonic Representation Hypothesis</strong>, suggesting that even if one model is trained on images and another on text, they may learn to organize their "understanding" of the world in a similar way. 
            More formally, 
          </p>

          <div class="column has-text-centered">
            <img src="static/images/platonic_rep_less_space_v2.jpg"
            width="30%"
            alt="Illustration of Plato's Allegory of the Cave."/>
          </div>
          <div class="subtitle has-text-centered is-6">
            Illustration of a shared reality being projected into different modalities. Taken from <a href="#huh2024platonic">[1]</a>.
          </div>
          <div class="box has-text-centered has-background-info-light">
            <p>
              Neural networks, trained with different objectives on different data and modalities, are converging to a <b>shared</b> statistical model of reality in their representation spaces. - <i>Huh et al. 2024</i>.
            </p>
          </div>

          <p>
            This, of course, is a reference to Plato's Allegory of the Cave <a href="#platoallegory">[2]</a>. 
            Fundamentally, all data is a projection of a shared reality - text, images, videos, audio, etc.
            As we observe more of it, we converge closer to the source of truth where they came from.  
          </p>
          <p>
            Modern image models and LLMs were evaluated for their alignment on static data like images and text, with the best performing models scoring an alignment close to 0.16 <a href="#huh2024platonic">[1]</a>. 
            This prompted the authors to wonder, "Is a score of 0.16 indicative of <b>strong alignment</b> [...] or does it signify poor alignment with major differences left to explain? We leave this as an open question."
          </p>

          <p>
            Our work addresses this resoundingly, showing that alignment for the exact same models can almost double by considering <i>dynamic inputs</i>, and achieve nearly 0.40 for video models, which more accurately reflect the true underlying representation from the real world. 
            Strong alignment was in these models all along; the data was just too impoverished to reveal it!
          </p>


         </div>
        
        <h3 class="title is-4">How Do We Measure Alignment?</h3>
        <div class="content has-text-justified">
          <p>
            To quantify how "similar" two representation spaces are, we need a metric that can compare their geometric structures without requiring them to be in the same coordinate system. We use a method called <strong>Mutual $k$-Nearest Neighbors (k-NN) Alignment</strong>. The intuition is simple: if two spaces are well-aligned, then points that are close to each other in one space should also be close to each other in the other space.
          </p>

          <p>
            While the spaces need not be in the same coordinate system, we need a dataset of paired video and text captions to link the two modalities together. 
            We utilize two datasets of 1024 pairs: the VaTeX dataset, consisting of 10-second YouTube clips captioned by 10 different English annotators, and the Perception Encoder Video Dataset (PVD) (<a href="#vatex">[3]</a>, <a href="#pvd">[4]</a>).
          </p>
        </div>

         <div class="column has-text-centered">
            <img src="static/images/sketch3.png"
            width="80%"
            center
            alt="Our pipeline for measuring alignment."/>
         </div>

        <h3 class="subtitle has-text-centered is-6">
            Our pipeline for measuring alignment. We encode videos and their corresponding captions, build nearest-neighbor graphs in each embedding space, and measure the overlap between these graphs.
        </h3>

        <div class="content has-text-justified">
          <p>Here's how it works for a dataset of paired videos and text captions:</p>
          <ol>
            <li>We encode all videos using a vision model and all captions using a text model, sampling varying amounts of information from each input.</li>
            <li>In each space, we build a nearest-neighbor graph, where each item (a video or caption) is connected to its $k$ closest neighbors.</li>
            <li>The alignment score is the fraction of edges that are shared between the two graphs. For example, if video A is a neighbor of video B, is caption A also a neighbor of caption B?</li>
          </ol>
          <p>A higher score means the neighborhood structures are more consistent across the two modalities, indicating stronger alignment. This metric is powerful because it's "zero-shot"‚Äîit requires no training or fine-tuning to compare the models.</p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Video Representations are Strongly Aligned with Text</h2>
         <div class="content has-text-justified">
          <p>
            We first measure alignment of various video models against Gemma-2 9B-IT on the VaTeX dataset below. 
            We test video models in three different settings.
            For image models, we consider two variants
            <ul>
              <li>
                We simply use a single (first) frame, denoted as <i>image model on image</i> üü©.
              </li>
              <li>
                To introduce naive temporal dynamics, we averaged the image features across 8 frames in the temporal dimension. We denote the resulting method as <i>image model on video</i> ‚ö†Ô∏è.
              </li>
            </ul>
            We also consider models trained on videos and adapt their native resolution üîµ.
            We found the following results: 
            <ul>
              <li>
                Pure image-only models (üü©) have less than 20% alignment on VaTeX, in line with the setting in Huh et. al. 2024.
              </li>
              <li>
                The strongest and most aligned models are video models (üîµ) (especially the self-supervised VideoMAEv2).
              </li>
              <li>
                Frame averaging image models is an effective baseline (üü© -> ‚ö†Ô∏è), especially for the motion-intensive SSv2 dataset.
              </li>
              <li>
                Recent, stronger LLMs (Gemma-2) are better aligned with visual models than weaker ones (Llama, Gemma-1).
              </li>
            </ul>
          </p>
        </div>
      </div>
    </div>
    <div class="column is-full-width">
      <div id="image-carousel" class="carousel results-carousel" style="width: 90%; margin: auto;">
        <div class="item">
          <img src="static/images/mainfig-1.png" alt="Main figure 1">
          <h3 class="subtitle has-text-centered is-6">Our work shows that as video representations become stronger, they become more closely aligned with text.
            <!-- We find that large video models (üîµ) and image models that average over frames (‚ö†Ô∏è) achieve the highest alignment and downstream performance, outperforming simple image-only models (üü©Ô∏è). -->
          </h3>
          <p></p>
        </div>
        <div class="item">
          <img src="static/images/mainfig-2.png" alt="Main figure 2">
          <h3 class="subtitle has-text-centered is-6">Pure image-only models (üü©) have less than 20% alignment on VaTeX, in line with the setting in Huh et. al. 2024.</h3>
        </div>
        <div class="item">
          <img src="static/images/mainfig-3.png" alt="Main figure 3">
          <h3 class="subtitle has-text-centered is-6">The strongest and most aligned models are video models (üîµ) (especially the self-supervised VideoMAEv2).</h3>
        </div>
        <div class="item">
          <img src="static/images/mainfig-4.png" alt="Main figure 4">
          <h3 class="subtitle has-text-centered is-6">Frame averaging image models is an effective baseline (üü© -> ‚ö†Ô∏è), especially for the motion-intensive SSv2 dataset.
          </h3>
        </div>
        <div class="item">
          <img src="static/images/mainfig-5.png" alt="Main figure 5">
          <h3 class="subtitle has-text-centered is-6">Recent, stronger LLMs (Gemma-2) are better aligned with visual models than weaker ones (Llama, Gemma-1).</h3>
        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            In the original PRH paper, the authors postulate the following claim:
          </p>
          <div class="box has-text-centered has-background-info-light">
            <p>
              Different models will converge to the same representation when the <span style="color: red;">input signals are sufficiently high information</span> and the models are sufficiently high capacity. - <i>Huh et. al., 2024</i>. 
            </p>
          </div>
          <p>
            Our results above reinforce that the previously observed low alignment was a result of the <b>input signals</b> being too impoverished, and that these models were sufficiently high capacity to begin with!
          </p>
        </div>
        <h3 class="title is-4">Embeddings are not Temporally Sensitive Yet</h3>
        <div class="content has-text-justified">
          <p>Seeing as we have dynamic inputs now, we can evaluate how dynamic our representations are. 
            We evaluate this on two datasets: the synthetic <i>Test of Time</i> and the challenging, long-form <i>VideoComp</i>.
            On this second dataset, when we compute alignment against positive captions vs. temporally reordered negatives in text space, it drops slightly but not by much. 
            This shows that there is still room for greater temporal understanding in our embeddings.  
          </p>
        </div>
        <div class="columns is-centered is-vcentered">
          <div class="column is-two-fifths">
            <img src="static/images/videocomp-sample.png" alt="Sample from VideoComp dataset."/>
          </div>
          <div class="column is-three-fifths">
            <img src="static/images/videocomp_pos_neg_tempreorder.png"
            alt="Video embeddings are sensitive to temporal caption reordering on VideoComp."/>
          </div>
        </div>
        <h3 class="subtitle has-text-centered is-6">
          (Left) A sample from the VideoComp dataset <a href="#videocomp">[5]</a>. (Right) Video embeddings are sensitive to temporal caption reordering.
        </h3>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Scaling Properties of Videos and Text</h2>
         <div class="content has-text-justified">
          <p>
            We find that video-text alignment can improve dramatically depending on the amount of data provided at <em>test time</em>. 
          </p>

          <p>
            As shown below, increasing the number of frames sampled from the video consistently improves alignment, as does increasing the number of captions.
            This makes sense: the more dynamic our inputs, the less we are approximating the true underlying reality. 
            Scaling frames benefits video models like VideoMAEv2 more than image models like DINOv2. 
          </p>
         </div>
      </div>
    </div>
    <div class="is-centered">
        <div class="column has-text-centered">
        <img src="static/images/vatex_scaling_side_by_side_enhanced.png"
        width="80%"
        center
        alt="Alignment scaling with frames and captions."/>
        </div>
        <h3 class="subtitle has-text-centered is-6">
        Alignment scales with the number of frames (left) and captions (right) at test time. Video models (like VideoMAEv2) benefit more from extra frames than image models (like DINOv2).
        </h3>
        <p></p>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
         <div class="content has-text-justified">
          <p>
            We found that even <i>artificially synthesized</i> caption descriptions from a single long video caption using an LLM can obtain alignment that is higher than that of the source caption.
            Namely, we find that our test time scaling approach improves alignment on the PVD dataset, which does not naturally come with multiple diverse captions, across a wide variety of models.
          </p>
        </div> 
      </div>
    </div>
    <div class="is-centered">
        <div class="column has-text-centered">
        <img src="static/images/pvd_scaling_linspace_center.png"
        width="80%"
        center
        alt="Alignment scaling with frames and captions for PVD."/>
        </div>
        <h3 class="subtitle has-text-centered is-6">
        Alignment scales with the number of frames (left) and captions (right) at test time. Video models (like VideoMAEv2) benefit more from extra frames than image models (like DINOv2).
        </h3>
        <p></p>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
         <div class="content has-text-justified">
          <p>
            We can quantify this dependence using neural scaling laws similar to the ones proposed in the Chinchilla paper <a href="chinchilla">[6]</a>. 
            We found that a saturation-based model fits our observations remarkably well:
          </p>
          <p style="text-align: center;">$$ \text{score}(\mathbf{n}_f, \mathbf{n}_c) = S_{\infty} - (C_f \mathbf{n}_f^{-\alpha} + C_c \mathbf{n}_c^{-\beta}) $$</p>
          <p>
            Here, $\mathbf{n}_f$ and $\mathbf{n}_c$ are the number of frames and captions. The formula suggests that alignment approaches a maximum, "infinite data" score ($S_{\infty}$) as we add more data. 
            The score is penalized by two terms that decay with the number of frames ($C_f \mathbf{n}_f^{-\alpha}$) and captions ($C_c \mathbf{n}_c^{-\beta}$). 
            This model fits our empirical data with high fidelity (e.g., $R^2 > 0.97$ for VideoMAEv2), demonstrating that the benefit of additional test-time data is predictable.
          </p>
          <p style="text-align: center;">$$ \text{VideoMAEv2}_{\text{score}}(\mathbf{n}_f,\mathbf{n}_c) = 0.41 - (0.15\mathbf{n}_f^{-0.75} + 0.13\mathbf{n}_c^{-1.30}) $$</p>
          <p style="text-align: center;">$$ \text{DINOv2}_{\text{score}}(\mathbf{n}_f,\mathbf{n}_c) = 0.37 - (0.05\mathbf{n}_f^{-1.76} + 0.13\mathbf{n}_c^{-1.40}) $$</p>
          <p>
            These fitted parameters provide insight into each model's unique behaviors. Notably, the frame exponent ($\alpha$) for DINOv2 is over double that of VideoMAEv2, while the caption coefficients and exponents ($C_c, \beta$) remain comparable, highlighting the video model's stronger ability to leverage temporal information from additional frames to improve alignment.
          </p> 
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Alignment as a Zero-Shot Video Probe</h2>
        <div class="content has-text-justified">
          <p>
            If this emergent alignment is meaningful, it should correlate with how well a video model performs on other video tasks. 
            We tested this by comparing the alignment scores of various self-supervised video models (trained without text) to their performance on a range of downstream tasks, from semantic to low-level geometric tasks.
          </p>
        </div>
      </div>
    </div>
    <div class="is-centered">
        <div class="column has-text-centered">
            <img src="static/images/corr_downstream_subset.png"
            width="80%"
            center
            alt="Correlation between alignment and downstream task performance."/>
        </div>
        <h3 class="subtitle has-text-centered is-6">
            Stronger video-text alignment correlates with better performance on both semantic (action classification) and geometric (tracking, depth) downstream tasks.
        </h3>
        <p></p>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>
            As shown above, we found a strong positive correlation. 
            Models that align better with text also tend to perform better on semantic tasks like action classification (on datasets like Kinetics and SSv2) and, surprisingly, even on non-semantic, geometric tasks like depth estimation, camera pose estimation, and object tracking.
            Point tracking remains an outlier, pointing to more room for video models to improve in.
          </p>
          <p>
            This suggests that video-text alignment can serve as a powerful, zero-shot indicator of a video model's general capabilities. Evaluating large video models can be expensive, often requiring extensive fine-tuning on multiple benchmarks. Platonic alignment provides a cheap and fast alternative by simply comparing a video model's representations with a standard text model on a small held out set.
            We can get a strong signal about its potential performance on a wide array of tasks, saving significant time and computational resources.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>In this work, we extend the Platonic Representation Hypothesis to the temporal domain, demonstrating that video-text alignment scores can double simply by using more video frames and diverse captions at test time‚Äîwithout any retraining. We quantify this with a highly accurate saturation-based scaling law, confirming that native video models are better equipped to leverage temporal information than static encoders.</p>
          <p>Our analysis reveals that this alignment score is a powerful zero-shot indicator of a model's downstream performance on both semantic and geometric tasks. However, it also highlights the limitations of current video models, some of which are still outperformed by simpler frame-averaging approaches. As video models continue to evolve, particularly generative ones, an open question remains: how can we best harness their latent representations for a deeper understanding of our dynamic world?</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop content">
      <h2 class="title">Related Literature</h2>
      <ol>
        <li id="huh2024platonic">
            Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. <i>The Platonic Representation Hypothesis.</i> ICML 2024.
        </li>
        <li id="platoallegory">
            Plato. <i>Allegory of the Cave.</i> 375 BC.
        </li>
        <li id="vatex">
            Xin Wang and Jiawei Wu and Junkun Chen and Lei Li and Yuan-Fang Wang and William Yang Wang. <i>VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research.</i> ICCV 2019.
        </li>
        <li id="pvd">
            Daniel Bolya et al, <i>Perception Encoder: The best visual embeddings are not at the output of the network.</i> arXiv preprint arXiv:2504.13181, 2025.
        </li>
        <li id="videocomp">
            Dahun Kim et al, <i>VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models</i>. CVPR 2025.
        </li>
        <li id="chinchilla">
            Jordan Hoffman et al, <i>Training Compute-Optimal Large Language Models.</i> NeurIPS 2022.
        </li>
      </ol>
    </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhu2025dynamic,
      title={Dynamic Reflections: Probing Video Representations with Text Alignment}, 
      author={Tyler Zhu and Tengda Han and Leonidas Guibas and Viorica PƒÉtrƒÉucean and Maks Ovsjanikov},
      journal={arXiv preprint arXiv:2511.02767},
      year={2025},
}</code></pre>
</div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template is graciously borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
