<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="We explore how video and text representations align, finding that more data at test timeâ€”more frames and more captionsâ€”leads to stronger alignment, which in turn correlates with better performance on downstream tasks.">
  <meta name="keywords" content="videos, LLMs, video representation, text alignment, representation learning, multimodal learning, video understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Dynamic Reflections: Probing Video Representations with Text Alignment</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css" integrity="sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi" crossorigin="anonymous">

  <!-- The loading of KaTeX is deferred to speed up page rendering -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js" integrity="sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ" crossorigin="anonymous"></script>

  <!-- To automatically render math in text elements, include the auto-render extension: -->
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          // customised options
          // â€¢ auto-render specific keys, e.g.:
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          // â€¢ rendering keys, e.g.:
          throwOnError : false
        });
    });
</script>

</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Dynamic Reflections: Probing Video Representations with Text Alignment</h1>

          <!-- <h2 class="is-size-5 is-centered"><strong>ICML 2025</strong></h2> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tylerzhu.com">Tyler Zhu</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://tengdahan.github.io/">Tengda Han</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://geometry.stanford.edu">Leonidas Guibas</a><sup>2</sup>,
            </span>
            <span class="author-block">
              Viorica PÄƒtrÄƒucean<sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.lix.polytechnique.fr/~maks/">Maks Ovsjanikov</a><sup>2</sup>
            </span>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Princeton University</span>,
            <span class="author-block"><sup>2</sup>Google DeepMind</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Work done while at Google DeepMind</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2511.02767"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/eBmAAdWrrAs"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <!-- <a href="https://github.com/google-deepmind/video-prh" -->
                <a 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming)</span>
                </a>
              </span>
              <!-- Slides Link. -->
              <!-- <span class="link-block">
                <a href="./static/cvpr23_pareprop_slides.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span> -->
              <!-- Poster Link. -->
              <span class="link-block">
                <!-- <a href="./static/merv_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-clipboard"></i>
                  </span>
                  <span>Poster</span>
                </a>
 -->              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="column has-text-centered">
        <img src="static/images/teaser.png" style="width:80%; display: block; margin: auto;" />
      </div>
      <h2 class="subtitle has-text-centered">
        Recent work has shown that models trained independently on different data types, like images and text, learn surprisingly similar internal structures. 
        We conduct the first comprehensive study of video and text alignment: more data at test time dramatically improves alignment, completely training-free!
        <!-- We extend this analysis to dynamic inputs, namely video and multiple sets of captions, finding that richer data significantly improves alignment. -->
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types.
            While significant progress has been made in aligning images with text, the temporal nature of <i>video</i> data remains largely unexplored in this context. 
            In this work, we conduct the <b>first comprehensive study of video-text representation alignment</b>, probing the capabilities of modern video and language encoders. 
            Our findings reveal several key insights. 
            First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data <i>provided at test time</i>, especially when using state-of-the-art video encoders. 
            We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations.
            Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to <i>general-purpose video</i> representation and understanding.
            Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. 
            Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data.
            <!-- The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. -->
<!-- While significant progress has been made in aligning images with text, the temporal nature of \textit{video} data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data \textit{provided at test time}, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to \textit{general-purpose} video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data.
            We explore how video and text representations align, finding that more data at test timeâ€”more frames and more captionsâ€”leads to stronger alignment, which in turn correlates with better performance on downstream tasks.
        We discovered that the richness of data at test time is key to closing the gap between different modalities.
        More frames and more diverse captions significantly improve alignment, which in turn correlates with better performance on a wide range of video understanding tasks. -->
        <!-- <figcaption>Our work shows that video representations are strongly aligned with text. We find that large video models (ðŸ”µ) and image models that average over frames (ðŸŸ ) achieve the highest alignment and downstream performance, outperforming simple image-only models (ðŸŸ¢).</figcaption> -->
          </p>
 
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Background: The Platonic Representation Hypothesis</h2>
         <div class="content has-text-justified">
          <p>
            A fascinating idea in machine learning is the <strong>Platonic Representation Hypothesis (PRH)</strong> <a href="#huh2024platonic">[1]</a>. It suggests that as we train larger and more capable neural networks on diverse data, their internal representations start to look alike. Even if one model is trained on images and another on text, they begin to organize their "understanding" of the world in a similar way. This shared structure is called a "Platonic representation."
          </p>
          <p>
            This has been demonstrated for static data like images and text <a href="#maniparambil2024vision">[2]</a>. But what about video? Video adds the dimension of time, which contains rich information about dynamics, causality, and motion. Does the PRH extend to this more complex, dynamic domain? That's the central question our work explores.
          </p>
         </div>
        
        <h3 class="title is-4">How Do We Measure Alignment?</h3>
        <div class="content has-text-justified">
          <p>
            To quantify how "similar" two representation spaces are, we need a metric that can compare their geometric structures without requiring them to be in the same coordinate system. We use a method called <strong>Mutual $k$-Nearest Neighbors (k-NN) Alignment</strong>. The intuition is simple: if two spaces are well-aligned, then points that are close to each other in one space should also be close to each other in the other space.
          </p>
        </div>

         <div class="column has-text-centered">
            <img src="static/images/sketch3.png"
            width="80%"
            center
            alt="Our pipeline for measuring alignment."/>
         </div>

        <h3 class="subtitle has-text-centered is-6">
            Our pipeline for measuring alignment. We encode videos and their corresponding captions, build nearest-neighbor graphs in each embedding space, and measure the overlap between these graphs.
        </h3>

        <div class="content has-text-justified">
          <p>Here's how it works for a dataset of paired videos and text captions:</p>
          <ol>
            <li>We encode all videos using a vision model and all captions using a text model, creating two distinct embedding spaces.</li>
            <li>In each space, we build a nearest-neighbor graph, where each item (a video or caption) is connected to its $k$ closest neighbors.</li>
            <li>The alignment score is the fraction of edges that are shared between the two graphs. For example, if video A is a neighbor of video B, is caption A also a neighbor of caption B?</li>
          </ol>
          <p>A higher score means the neighborhood structures are more consistent across the two modalities, indicating stronger alignment. This metric is powerful because it's "zero-shot"â€”it requires no training or fine-tuning to compare the models.</p>
        </div>
      </div>
    </div>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Key Result 1: More Data at Test Time = Better Alignment</h2>
         <div class="content has-text-justified">
          <p>
            Our first major finding is that video-text alignment is highly dependent on the amount of data provided at <em>test time</em>. We found this to be true for both the visual (number of frames) and textual (number of captions) data. This "test-time scaling" effect is surprisingly strong and consistent.
          </p>
         </div>
      </div>
    </div>
    <div class="is-centered">
        <div class="column has-text-centered">
        <img src="static/images/vatex_scaling_side_by_side_enhanced.png"
        width="80%"
        center
        style="border: 1px solid rgba(0, 0, 0, 0.2);"
        alt="Alignment scaling with frames and captions."/>
        </div>
        <h3 class="subtitle has-text-centered is-6">
        Alignment scales with the number of frames (left) and captions (right) at test time. Video models (like VideoMAEv2) benefit more from extra frames than image models (like DINOv2).
        </h3>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
         <div class="content has-text-justified">
          <p>
            As shown above, increasing the number of frames from a video consistently improves alignment. This makes sense: more frames provide a richer temporal context. Interestingly, dedicated video models like VideoMAEv2 benefit much more from additional frames than image models applied to video, like DINOv2.
          </p>
          <p>
            Perhaps more surprisingly, using more captions for the same video also provides a significant boost. On the VaTeX dataset, which has 10 different human-written captions per video, we found that using all 10 captions improves alignment by 60% on average compared to using just one. This suggests that the diversity of language in multiple descriptions helps to ground the visual concepts more robustly. We even found that using an LLM to generate multiple synthetic captions from a single detailed one can improve alignment over using the original caption alone.
          </p>
         </div>
        <h3 class="title is-4">Predictable Scaling at Test Time</h3>
        <div class="content has-text-justified">
          <p>This relationship between test-time data and alignment is so consistent that we can model it with a "scaling law." We found that a saturation-based model fits our observations remarkably well:</p>
          <p style="text-align: center;">$$ \text{score}(n_f, n_c) = S_{\infty} - (C_f n_f^{-\alpha} + C_c n_c^{-\beta}) $$</p>
          <p>Here, $n_f$ and $n_c$ are the number of frames and captions. The formula suggests that alignment approaches a maximum, "infinite data" score ($S_{\infty}$) as we add more data. The score is penalized by two terms that decay with the number of frames ($C_f n_f^{-\alpha}$) and captions ($C_c n_c^{-\beta}$). This model fits our empirical data with high fidelity (e.g., $R^2 > 0.97$ for VideoMAEv2), demonstrating that the benefit of additional test-time data is predictable. This is conceptually similar to scaling laws for model training, but applied for the first time to inference.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">The Power of Temporal Dynamics</h2>
        <div class="content has-text-justified">
          <p>A key question is whether dedicated video models, which are designed to understand temporal dynamics, show better emergent alignment than powerful image models simply applied frame-by-frame. Our results suggest they do.</p>
          <p>When we provide only a single frame from a video, top-tier image models like DINOv2 often show stronger alignment than video models. This is expected, as they are specialists in static scenes. However, as we increase the number of frames, the picture changes dramatically. Video models like VideoMAEv2 show a much steeper improvement in alignment score as more frames are added. Their ability to process spatio-temporal information allows them to build a more comprehensive and semantically rich representation that aligns better with language.</p>
          <p>This suggests that the temporal dynamics in video are not just noise, but a powerful signal for grounding semantics. Learning about motion, causality, and object interactions over time appears to help models build an internal structure that is more congruent with the concepts expressed in natural language. We also tested this on datasets designed to probe temporal understanding, like Test of Time and VideoComp, and found that while current models show some sensitivity to temporal order, there is still significant room for improvement.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Key Result 2: Alignment is a Proxy for Model Quality</h2>
        <div class="content has-text-justified">
          <p>If this emergent alignment is meaningful, it should correlate with how well a video model performs on other tasks. We tested this by comparing the alignment scores of various self-supervised video models (trained without text) to their performance on a range of downstream tasks.</p>
        </div>
      </div>
    </div>
    <div class="is-centered">
        <div class="column has-text-centered">
            <img src="static/images/corr_downstream_subset.png"
            width="80%"
            center
            style="border: 1px solid rgba(0, 0, 0, 0.2);"
            alt="Correlation between alignment and downstream task performance."/>
        </div>
        <h3 class="subtitle has-text-centered is-6">
            Stronger video-text alignment correlates with better performance on both semantic (action classification) and geometric (tracking, depth) downstream tasks.
        </h3>
    </div>
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p>As shown above, we found a strong positive correlation. Models that align better with text also tend to perform better on semantic tasks like action classification (on datasets like Kinetics and SSv2) and, surprisingly, even on non-semantic, geometric tasks like depth estimation, camera pose estimation, and object tracking.</p>
          <p>This is a crucial finding. It suggests that video-text alignment can serve as a powerful, zero-shot indicator of a video model's general capabilities. Evaluating large video models is notoriously expensive, often requiring extensive fine-tuning on multiple benchmarks. Our alignment metric provides a cheap and fast alternative: simply by measuring how well a video model's representations align with a standard text model, we can get a strong signal about its potential performance on a wide array of tasks, saving significant time and computational resources.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>Our work is the first comprehensive study of emergent alignment between video and text representations. We've shown that the principles of the Platonic Representation Hypothesis extend to the dynamic world of video. Our key takeaways are:</p>
          <ol>
            <li><strong>Test-time data matters:</strong> Richer visual and textual data at inference time significantly boosts alignment. This holds for both the number of video frames and the number of diverse captions.</li>
            <li><strong>Predictable scaling:</strong> This relationship follows predictable scaling laws, which can help in designing data collection and model evaluation strategies.</li>
            <li><strong>Alignment as a proxy for quality:</strong> Stronger alignment with text is a good indicator of a video model's general-purpose representation capabilities, across both semantic and geometric tasks.</li>
          </ol>
          <p>Overall, we introduce video-text alignment as an informative, zero-shot way to probe the representation power of different encoders for spatio-temporal data. The temporal dimension in video appears to provide a powerful signal for grounding semantics, opening up exciting avenues for future research in multimodal learning.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
    <div class="container is-max-desktop content">
      <h2 class="title">Related Literature</h2>
      <ol>
        <li id="huh2024platonic">
            Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. "The Platonic Representation Hypothesis." In <i>Proceedings of the 41st International Conference on Machine Learning</i>, 2024.
        </li>
        <li id="maniparambil2024vision">
            Jishnu Maniparambil, Matthew Gwilliam, Puyuan Gu, Suraj Srinivas, Abhinav Shrivastava, and Sudeep Mallya. "Vision-Language Models are Zero-Shot Representation Aligners." <i>arXiv preprint arXiv:2402.19427</i>, 2024.
        </li>
      </ol>
    </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhu2025dynamic,
      title={Dynamic Reflections: Probing Video Representations with Text Alignment}, 
      author={Tyler Zhu and Tengda Han and Leonidas Guibas and Viorica PÄƒtrÄƒucean and Maks Ovsjanikov},
      journal={arXiv preprint arXiv:2511.02767},
      year={2025},
}</code></pre>
</div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website template is graciously borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
